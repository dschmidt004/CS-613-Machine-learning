{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8883071553228621\n",
      "Recall:  0.8821490467937608\n",
      "fMeasure:  0.8852173913043478\n",
      "Accuracy:  91.3894324853229 %\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Problem 5\n",
    "##\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from pprint import pprint\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "data = pd.read_csv('spambase.data')\n",
    "data = np.array(data)\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# split into training and test sets\n",
    "trainingIndex = -(-2 * len(data) // 3)\n",
    "trainingData = data[0:trainingIndex]\n",
    "testData = data[trainingIndex:]\n",
    "\n",
    "# standardize data\n",
    "mean = np.mean(trainingData, axis=0)\n",
    "std = np.std(trainingData, axis=0, ddof=1)\n",
    "sXTrainingData = (trainingData[:,0:57]-mean[0:57])/std[0:57]\n",
    "sXTestData = (testData[:,0:57]-mean[0:57])/std[0:57]\n",
    "trainingData = trainingData.astype(int)\n",
    "testData = testData.astype(int)\n",
    "\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self, feature=None, threshold=None, left=None, right=None, *, value=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if (\n",
    "            depth >= self.max_depth\n",
    "            or n_labels == 1\n",
    "            or n_samples < self.min_samples_split\n",
    "        ):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
    "\n",
    "        # greedily select the best split according to information gain\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_thresh = threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        # parent loss\n",
    "        parent_entropy = entropy(y)\n",
    "\n",
    "        # generate split\n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # information gain is difference in loss before vs. after split\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)\n",
    "        if len(most_common) == 0:\n",
    "            return None\n",
    "        return most_common[0][0]\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def evaluateModel(y_true, predictions):\n",
    "        \n",
    "        truePositives = 0\n",
    "        trueNegatives = 0\n",
    "        falsePositives = 0\n",
    "        falseNegatives = 0\n",
    "\n",
    "        for n in range(len(y_true)):\n",
    "            if y_true[n] == 1:\n",
    "                if predictions[n] == 1:\n",
    "                    truePositives += 1\n",
    "                else:\n",
    "                    falseNegatives += 1\n",
    "            else:\n",
    "                if predictions[n] == 0:\n",
    "                    trueNegatives += 1\n",
    "                else:\n",
    "                    falsePositives += 1\n",
    "\n",
    "        precision = truePositives/(truePositives + falsePositives)\n",
    "        recall = truePositives/(truePositives + falseNegatives)\n",
    "        fMeasure = (2 * precision * recall)/(precision + recall)\n",
    "        accuracy = ((truePositives+trueNegatives)/(truePositives+trueNegatives +\\\n",
    "            falsePositives+falseNegatives)) * 100\n",
    "\n",
    "        print(\"Precision: \",precision)\n",
    "        print(\"Recall: \",recall)\n",
    "        print(\"fMeasure: \",fMeasure)\n",
    "        print(\"Accuracy: \",accuracy,\"%\")\n",
    "\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    X_train = sXTrainingData\n",
    "    X_test = sXTestData\n",
    "    y_train = trainingData[:,57]\n",
    "    y_test = testData[:,57]\n",
    "\n",
    "    clf = DecisionTree(max_depth=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    evaluateModel(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
